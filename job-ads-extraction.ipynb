{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1403062,"sourceType":"datasetVersion","datasetId":820137},{"sourceId":6488828,"sourceType":"datasetVersion","datasetId":3749643},{"sourceId":7293228,"sourceType":"datasetVersion","datasetId":4230047}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport time\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk import ngrams\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom bs4 import BeautifulSoup\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\n\n# Transformers and related libraries\nimport transformers\nfrom transformers import pipeline, AutoTokenizer, AutoModel\n\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:15:51.420298Z","iopub.execute_input":"2023-12-31T17:15:51.421100Z","iopub.status.idle":"2023-12-31T17:16:11.363373Z","shell.execute_reply.started":"2023-12-31T17:15:51.421067Z","shell.execute_reply":"2023-12-31T17:16:11.362236Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Load the job ads dataset and ESCO skills ontology\nI removed duplicates from the job ads, the dataset is still very big, I won't be using all the entries.\n<br>For the ESCO skills ontology, I will only be using the preferred labels and consider all entries as skills, even though ESCO divides them into subcategories.","metadata":{}},{"cell_type":"code","source":"JOBS_FP = '/kaggle/input/indeed-job-posting-dataset/home/sdf/marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv'\nESCO_SKILLS_FP = '/kaggle/input/esco-skills/skills_en.csv'\n\ndf = pd.read_csv(JOBS_FP)\nprint(\"N before: \", len(df))\ndf.drop_duplicates(subset=['Job Description'], keep='first', inplace=True)\nprint(\"N after: \", len(df))\n\nesco_df = pd.read_csv(ESCO_SKILLS_FP)\n# Remove \"(text)\" occurences\nesco_df['label_cleaned'] = esco_df['preferredLabel'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x).strip())\n# Count words in skills after cleaning\nesco_df['word_cnt'] = esco_df['label_cleaned'].apply(lambda x: len(str(x).split()))\nesco_df = pd.DataFrame(esco_df, columns=['label_cleaned', 'altLabels', 'word_cnt'])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:16:11.365416Z","iopub.execute_input":"2023-12-31T17:16:11.366239Z","iopub.status.idle":"2023-12-31T17:16:15.204938Z","shell.execute_reply.started":"2023-12-31T17:16:11.366201Z","shell.execute_reply":"2023-12-31T17:16:15.204116Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"N before:  30002\nN after:  24714\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Skills lengths**<br>\nI checked the word count frequency in ESCO skills, most of them consist of 3 words and just by looking you could say almost no skills consist of more than 9 words. I planned to use this for determining the maximal ngram length, but in the end opted for whole sentence representation","metadata":{}},{"cell_type":"code","source":"max_value = max(esco_df.word_cnt)\nbins = range(1, max_value + 2)\n\nplt.hist(esco_df.word_cnt, bins=bins)\nplt.xticks(range(1, max_value + 2))\n\nplt.xlabel('Word count')\nplt.ylabel('Frequency')\nplt.title('ESCO Skills length')\nplt.savefig('esco_word_count.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:16:15.206140Z","iopub.execute_input":"2023-12-31T17:16:15.206488Z","iopub.status.idle":"2023-12-31T17:16:15.611460Z","shell.execute_reply.started":"2023-12-31T17:16:15.206455Z","shell.execute_reply":"2023-12-31T17:16:15.610493Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"class EscoDataset(Dataset):\n    def __init__(self, df, skill_col, backbone):\n        texts = df\n        self.tokenizer = AutoTokenizer.from_pretrained(backbone)\n        self.texts = texts[skill_col].values.tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        res = self.tokenizer(\n            self.texts[idx],\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=20\n        )\n        return {k:v[0] for k,v in res.items()}\n\n    \nclass ClsPool(nn.Module):\n    def forward(self, x):\n        # batch * num_tokens * num_embedding\n        return x[:, 0, :]    \n\n    \nclass BertModel(nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n        \n        self.backbone_name = backbone\n        self.backbone = AutoModel.from_pretrained(backbone)\n        self.pool = ClsPool()\n    \n    def forward(self, x):\n        x = self.backbone(**x)[\"last_hidden_state\"]\n        x = self.pool(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:16:15.614300Z","iopub.execute_input":"2023-12-31T17:16:15.615043Z","iopub.status.idle":"2023-12-31T17:16:15.624432Z","shell.execute_reply.started":"2023-12-31T17:16:15.615004Z","shell.execute_reply":"2023-12-31T17:16:15.623433Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## JobBERT\nJobBERT is a BERT model pre-trained on job propositions, it came from a paper where they concluded that a domain-specific pretrained model outperformed the non-adapted versions and published their model on [Huggingface](https://huggingface.co/jjzha/jobbert-base-cased). I will use it to create embeddings of ESCO skills, then embed the job postings and find relevant ESCO skills using vector similarity","metadata":{}},{"cell_type":"code","source":"backbone = 'jjzha/jobbert-base-cased'\nemb_label = 'jobbert'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset and Dataloader\nds = EscoDataset(esco_df, 'label_cleaned', backbone)\ndl = DataLoader(ds, shuffle=False, batch_size=32)\n\n# Build custom model\nmodel = BertModel(backbone)\nmodel.eval()\nmodel.to(device)\n\n# Get embeddings for each skill\nembs = []\nwith torch.no_grad():\n    for i, x in enumerate(dl):\n        x = {k:v.to(device) for k, v in x.items()}\n        out = model(x)\n        embs.extend(out.detach().cpu())\n# Add them to the DataFrame\nesco_df[emb_label] = embs","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:16:15.625769Z","iopub.execute_input":"2023-12-31T17:16:15.626132Z","iopub.status.idle":"2023-12-31T17:16:39.556599Z","shell.execute_reply.started":"2023-12-31T17:16:15.626096Z","shell.execute_reply":"2023-12-31T17:16:39.555564Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c9f73f05f449cb9150bb4c8053eee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/603 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff3178ee21e4000a56ffe3085d83ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e375e3485e14457eaec5a3af9be05709"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6821bf0d30f640fdb1a6b6c18cb7c9fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55316cc03d6c455eb1368a59fea35745"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_sentences(job):\n    \"\"\"\n    Given a raw html job description, parse it into sentences\n    by using nltk's sentence tokenization + new line splitting\n    \"\"\"\n    soup = BeautifulSoup(job, 'html.parser')\n    # Found some ads using unicode bullet points\n    for p in soup.find_all('p'):\n        p.string = p.get_text().replace(\"â€¢\", \"\")\n    text = soup.get_text()\n    st = sent_tokenize(text)\n    sentences = []\n    for sent in st:\n        sentences.extend([x for x in sent.split('\\n') if x !=''])\n    return sentences\n\ndef compute_similarity(vec, emb_type):\n    \"\"\"\n    Compute vector similarity for a given vec and all the ESCO skills embeddings.\n    If more embeddings were created, the type is specified by the input parameter.\n    Return the ESCO skill id with max similarity\n    \"\"\"\n    esco_embs = esco_df[emb_type]\n    sims = []\n    # Compute cosine similarities\n    for i, esco_vec in enumerate(esco_embs):\n        sims.append((i, cosine_similarity(vec, esco_vec.reshape(1, -1))))\n    # Return max similarity and esco skill index\n    idx, sim = max(sims, key=lambda x: x[1])\n    return idx, sim.item()\n\n\ndef compute_similarity_opt(emb_vec, emb_type):\n    \"\"\"\n    Compute vector similarity for a given vec and all the ESCO skills embeddings\n    by constructing a matrix from ESCO embeddings to process it faster.\n    Return the ESCO skill id with max similarity\n    \"\"\"\n    esco_embs = [x for x in esco_df[emb_type]]\n    esco_vectors = torch.stack(esco_embs)\n    # Normalize the stacked embeddings and the input vector\n    norm_esco_vectors = torch.nn.functional.normalize(esco_vectors, p=2, dim=1)\n    norm_emb_vec = torch.nn.functional.normalize(emb_vec.T, p=2, dim=0)\n    # Compute cosine similarities\n    cos_similarities = torch.matmul(norm_esco_vectors, norm_emb_vec)\n    # Return max similarity and esco skill index\n    sim, idx = torch.max(cos_similarities, dim=0)\n    return idx.item(), sim.item()\n\ndef compute_similarity_mat(emb_mat, emb_type):\n    esco_embs = [x for x in esco_df[emb_type]]\n    esco_vectors = torch.stack(esco_embs)\n    emb_vectors = torch.stack(emb_mat)\n    # Normalize the stacked embeddings and the input vectors\n    norm_esco_vectors = torch.nn.functional.normalize(esco_vectors, p=2, dim=1)\n    norm_emb_vecs = torch.nn.functional.normalize(emb_vectors.T, p=2, dim=0)\n    # Compute cosine similarities\n    cos_similarities = torch.matmul(norm_esco_vectors, norm_emb_vecs)\n    # Return max similarity and esco skill index\n    max_similarities, max_indices = torch.max(cos_similarities, dim=0)\n    return max_indices.numpy(), max_similarities.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:16:39.557806Z","iopub.execute_input":"2023-12-31T17:16:39.558088Z","iopub.status.idle":"2023-12-31T17:16:39.571509Z","shell.execute_reply.started":"2023-12-31T17:16:39.558065Z","shell.execute_reply":"2023-12-31T17:16:39.570528Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_embedding(x):\n    x = tokenizer(x, return_tensors='pt')\n    x = {k:v.to(device) for k, v in x.items()}\n    return model(x).detach().cpu()\n\ndef process_sentence(sent):\n    emb = get_embedding(sent)\n    return compute_similarity_opt(emb, emb_label)\n\ntokenizer = AutoTokenizer.from_pretrained(backbone)\nmodel = BertModel(backbone)\nmodel.to(device)\nmodel.eval()\n\n# Used in performance optimization and output example\njob_sample = df.iloc[15]['Job Description']\nthreshold = .8","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:16:39.572781Z","iopub.execute_input":"2023-12-31T17:16:39.573053Z","iopub.status.idle":"2023-12-31T17:16:40.208277Z","shell.execute_reply.started":"2023-12-31T17:16:39.573029Z","shell.execute_reply":"2023-12-31T17:16:40.207238Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Performance optimization\nBy improving the similarity calculation (using tensor operations) on one random job sample, the processing time for all the sentences went down from around 160 seconds to 2 seconds. (A different sample showed improvement from 300 to 5 seconds. The time improvement value is not exact but the improvement is significant)","metadata":{}},{"cell_type":"code","source":"sentences = get_sentences(job_sample)\n\n# Simple similarity\nsim_start_time = time.time()\nfor sent in sentences:\n    x = tokenizer(sent, return_tensors='pt')\n    x = {k:v.to(device) for k, v in x.items()}\n    emb = model(x).detach().cpu()\n    idx, sim = compute_similarity(emb.numpy(), emb_label)\n\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"One-by-one similarity execution time: {execution_time:.4f} seconds\")\n\n# Optimized similarity\nsim_start_time = time.time()\nfor sent in sentences:\n    x = tokenizer(sent, return_tensors='pt')\n    x = {k:v.to(device) for k, v in x.items()}\n    emb = model(x).detach().cpu()\n    idx, sim = compute_similarity_opt(emb, emb_label)\n\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"Optimized similarity execution time: {execution_time:.4f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T13:00:51.443567Z","iopub.execute_input":"2023-12-31T13:00:51.443915Z","iopub.status.idle":"2023-12-31T13:03:39.093580Z","shell.execute_reply.started":"2023-12-31T13:00:51.443889Z","shell.execute_reply":"2023-12-31T13:03:39.092597Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"One-by-one similarity execution time: 165.6036 seconds\nOptimized similarity execution time: 2.0319 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Further optimization\nI edited the method once again to compute the similarity between two matrices (matrix of sentence embeddings and a matrix of esco embeddings) in one operation. This further enhanced the processing time.","metadata":{}},{"cell_type":"code","source":"sentences = get_sentences(job_sample)\n\nsim_start_time = time.time()\nsent_embs = []\nfor sent in sentences:\n    x = tokenizer(sent, return_tensors='pt')\n    x = {k:v.to(device) for k, v in x.items()}\n    emb = model(x).detach().cpu()\n    sent_embs.append(emb.squeeze())\nidxs, sims = compute_similarity_mat(sent_embs, emb_label)\n# Calculate job description processing time\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:14.020296Z","iopub.execute_input":"2023-12-31T17:17:14.020704Z","iopub.status.idle":"2023-12-31T17:17:14.531888Z","shell.execute_reply.started":"2023-12-31T17:17:14.020673Z","shell.execute_reply":"2023-12-31T17:17:14.530871Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Execution time: 0.4979 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Example with ESCO mapping outputs**\n\nI am using a threshold parameter to filter out unlikely matches. We can see that the sentence-wise detection made some mistakes but the mapped skills are not entirely irrelevant. I also display speeds for both \"fast\" similarity calculation approaches","metadata":{}},{"cell_type":"code","source":"sim_start_time = time.time()\nres = []\nsentences = get_sentences(job_sample)\nfor sent in sentences:\n    idx, sim = process_sentence(sent)\n    if sim > threshold:\n        res.append((sent, esco_df.iloc[idx]['label_cleaned'], sim))\n\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")\n\nfor r in res:\n    print('=========================')\n    print(f\"sentence: {r[0]}\\nESCO skill:{r[1]}\\nSimilarity:{r[2]:.4f}\") ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:17.425643Z","iopub.execute_input":"2023-12-31T17:17:17.426414Z","iopub.status.idle":"2023-12-31T17:17:20.443496Z","shell.execute_reply.started":"2023-12-31T17:17:17.426378Z","shell.execute_reply":"2023-12-31T17:17:20.442492Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Execution time: 3.0104 seconds\n=========================\nsentence: Contribute to & maintain open source projects\nESCO skill:operate open source software\nSimilarity:0.8464\n=========================\nsentence: Familiar with agile methodologies\nESCO skill:Agile development\nSimilarity:0.8328\n=========================\nsentence: Experience with design and development of backend services\nESCO skill:implement front-end website design\nSimilarity:0.8274\n=========================\nsentence: Experience with software testing methodologies\nESCO skill:levels of software testing\nSimilarity:0.8499\n=========================\nsentence: Contributions to open-source projects\nESCO skill:Open source model\nSimilarity:0.8590\n","output_type":"stream"}]},{"cell_type":"code","source":"sim_start_time = time.time()\nres = []\nsent_embs = []\nsentences = get_sentences(job_sample)\nfor sent in sentences:\n    sent_embs.append(get_embedding(sent).squeeze())\n    \nidxs, sims = compute_similarity_mat(sent_embs, emb_label)\nfor i in range(len(idxs)):\n    if sims[i] > threshold:\n        res.append((sentences[i], esco_df.iloc[idxs[i]]['label_cleaned'], sims[i]))\n\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")\n\nfor r in res:\n    print('=========================')\n    print(f\"sentence: {r[0]}\\nESCO skill:{r[1]}\\nSimilarity:{r[2]:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:20.445426Z","iopub.execute_input":"2023-12-31T17:17:20.446341Z","iopub.status.idle":"2023-12-31T17:17:20.952747Z","shell.execute_reply.started":"2023-12-31T17:17:20.446309Z","shell.execute_reply":"2023-12-31T17:17:20.951750Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Execution time: 0.4991 seconds\n=========================\nsentence: Contribute to & maintain open source projects\nESCO skill:operate open source software\nSimilarity:0.8464\n=========================\nsentence: Familiar with agile methodologies\nESCO skill:Agile development\nSimilarity:0.8328\n=========================\nsentence: Experience with design and development of backend services\nESCO skill:implement front-end website design\nSimilarity:0.8274\n=========================\nsentence: Experience with software testing methodologies\nESCO skill:levels of software testing\nSimilarity:0.8499\n=========================\nsentence: Contributions to open-source projects\nESCO skill:Open source model\nSimilarity:0.8590\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## JobBERT and EscoXLMR\nThe author of the JobBERT model also created demos on Huggingface using his already-trained models that extract skills based on token classification. The code for the demo is available [here](https://huggingface.co/spaces/jjzha/skill_extraction_demo/tree/main). I modified the code so that the model outputs are checked agains a threshold for skill detection and then the filtered texts are encoded and mapped to ESCO skills using vector similarity with another threshold value.\n\nI tried both JobBERT and EscoXLMR but it seemed to me EscoXMLR had some problems with respresenting found spans correctly, moreover I am already using JobBERT embeddings so I opted for this model.","metadata":{}},{"cell_type":"code","source":"def get_classifiers(mtype):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if mtype == \"jobbert\":\n        token_skill_classifier = pipeline(model=\"jjzha/jobbert_skill_extraction\", aggregation_strategy=\"first\", device=device)\n        token_knowledge_classifier = pipeline(model=\"jjzha/jobbert_knowledge_extraction\", aggregation_strategy=\"first\", device=device)\n    elif mtype == \"xlmr\":        \n        token_skill_classifier = pipeline(model=\"jjzha/escoxlmr_skill_extraction\", aggregation_strategy=\"first\", device=device)\n        token_knowledge_classifier = pipeline(model=\"jjzha/escoxlmr_knowledge_extraction\", aggregation_strategy=\"first\", device=device)\n    else:\n        raise Exception(\"Unknown model name provided\")\n    return token_skill_classifier, token_knowledge_classifier\n\n\ndef extract_skills(job, token_skill_classifier, token_knowledge_classifier, out_treshold=.8, sim_threshold=.8):\n    \"\"\"\n    Function that processes outputs from pre-trained, ready to use models\n    that detect skills as a token classification task. There are two thresholds,\n    out_threshold for filtering model outputs and sim_threshold for filtering\n    based on vector similarity with ESCO skills\n    \"\"\"     \n    sentences = get_sentences(job)\n    pred_labels = []\n    res = []\n    skill_embs = []\n    skill_texts = []\n    for sent in sentences:\n        skills = ner(sent, token_skill_classifier, token_knowledge_classifier)\n        for entity in skills['entities']:\n            text = entity['word']\n            if entity['score'] > out_treshold:\n                skill_embs.append(get_embedding(text).squeeze())\n                skill_texts.append(text)\n                \n    idxs, sims = compute_similarity_mat(skill_embs, emb_label)\n    for i in range(len(idxs)):\n        if sims[i] > sim_threshold:\n            pred_labels.append(idxs[i])\n            res.append((skill_texts[i], esco_df.iloc[idxs[i]]['label_cleaned'], sims[i]))\n    return pred_labels, res\n\n\ndef aggregate_span(results):\n    new_results = []\n    current_result = results[0]\n\n    for result in results[1:]:\n        if result[\"start\"] == current_result[\"end\"] + 1:\n            current_result[\"word\"] += \" \" + result[\"word\"]\n            current_result[\"end\"] = result[\"end\"]\n        else:\n            new_results.append(current_result)\n            current_result = result\n\n    new_results.append(current_result)\n\n    return new_results\n\n\ndef ner(text, token_skill_classifier, token_knowledge_classifier):\n    output_skills = token_skill_classifier(text)\n    for result in output_skills:\n        if result.get(\"entity_group\"):\n            result[\"entity\"] = \"Skill\"\n            del result[\"entity_group\"]\n\n    output_knowledge = token_knowledge_classifier(text)\n    for result in output_knowledge:\n        if result.get(\"entity_group\"):\n            result[\"entity\"] = \"Knowledge\"\n            del result[\"entity_group\"]\n\n    if len(output_skills) > 0:\n        output_skills = aggregate_span(output_skills)\n    if len(output_knowledge) > 0:\n        output_knowledge = aggregate_span(output_knowledge)\n    \n    skills = []\n    skills.extend(output_skills)\n    skills.extend(output_knowledge)\n    return {\"text\": text, \"entities\": skills}","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:21.346064Z","iopub.execute_input":"2023-12-31T17:17:21.346764Z","iopub.status.idle":"2023-12-31T17:17:21.363873Z","shell.execute_reply.started":"2023-12-31T17:17:21.346730Z","shell.execute_reply":"2023-12-31T17:17:21.362752Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We can see that this approach catches much more skills as it works on token-level. Most of them are also correct","metadata":{}},{"cell_type":"code","source":"tsc, tkc = get_classifiers(\"jobbert\")\n\nstart_time = time.time()\n_, res = extract_skills(job_sample, tsc, tkc)\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")\nfor r in res:\n    print('=========================')\n    print(f\"text: {r[0]}\\nESCO skill:{r[1]}\\nSimilarity:{r[2]:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:23.446415Z","iopub.execute_input":"2023-12-31T17:17:23.447326Z","iopub.status.idle":"2023-12-31T17:17:32.019441Z","shell.execute_reply.started":"2023-12-31T17:17:23.447292Z","shell.execute_reply":"2023-12-31T17:17:32.018379Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"216109d4573e4342ab44fb333a19a0e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20d51e3759c43c699e364c96606daab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e097cb7ff14e78a06030e4cbe32dc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ce5785d3e9406591c520a98b4170f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ecc092c4bf4458d98891f52128c5319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868d73a6501c4dd7a3ff9638163faaad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca9d7249a07405484d5a76a4a8461b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa9d015ea1a4be287b9015d9db9beb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221f5099f0204ffca060133d0740de68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95bfa207435d46e18982bec1f82f9c27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb970c9dd34e4c1a9f73f9c1ff40cecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d56b4baa1883485585e04791f3690d2c"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Execution time: 1.3490 seconds\n=========================\ntext: self - direction\nESCO skill:self-promote\nSimilarity:0.8803\n=========================\ntext: building creative solutions to challenging problems\nESCO skill:create solutions to problems\nSimilarity:0.8554\n=========================\ntext: work in a team\nESCO skill:work in a landscape team\nSimilarity:0.9260\n=========================\ntext: Contribute to & maintain open source projects\nESCO skill:operate open source software\nSimilarity:0.8464\n=========================\ntext: open source projects\nESCO skill:Open source model\nSimilarity:0.9168\n=========================\ntext: software engineering\nESCO skill:computer engineering\nSimilarity:0.8377\n=========================\ntext: leading a team\nESCO skill:lead a team\nSimilarity:0.9468\n=========================\ntext: agile methodologies\nESCO skill:Agile development\nSimilarity:0.8723\n=========================\ntext: design and development of backend services\nESCO skill:implement front-end website design\nSimilarity:0.8291\n=========================\ntext: backend\nESCO skill:JavaScript\nSimilarity:0.8135\n=========================\ntext: Java\nESCO skill:Java\nSimilarity:1.0000\n=========================\ntext: Python\nESCO skill:Python\nSimilarity:1.0000\n=========================\ntext: Javascript\nESCO skill:JavaScript\nSimilarity:0.9068\n=========================\ntext: Solidity\nESCO skill:Solidity\nSimilarity:1.0000\n=========================\ntext: software testing methodologies\nESCO skill:software design methodologies\nSimilarity:0.9010\n=========================\ntext: open - source\nESCO skill:Open source model\nSimilarity:0.9089\n=========================\ntext: Jenkins\nESCO skill:Jenkins\nSimilarity:1.0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Small labelled data subset\nMy task was to label a subset of data, I found out that the task was harder than I expected so I ended up manually labelling only 4 randomly selected job ads. For the labelling approach, I chose to use index names of ESCO skills as labels. It is true that this might mean that some skills, that are present in the description will not be picked up, but the outputs will be more uniform.\n\nAnother approach I though about was to label the data on span-level with the help of a labeling tool to make it faster and easier. I would then use just the parts of text detected as skills instead of ESCO terms. This might have helped to better extract concrete skills rather than generalized ontology terms.","metadata":{}},{"cell_type":"code","source":"def calculate_metrics(preds, labels):\n    tp = 0\n    fp = 0\n    fn = 0\n    for k, v in preds.items():\n        target = labels[k] \n        # Calculate TP, FP, FN for the current entry\n        tp += sum(1 for i in range(len(v)) if v[i] in target)\n        fp += sum(1 for i in range(len(v)) if v[i] not in target)\n        fn += sum(1 for i in range(len(target)) if target[i] not in v)\n\n    # Calculate precision, recall, and F1-score for the current entry\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:32.021163Z","iopub.execute_input":"2023-12-31T17:17:32.021458Z","iopub.status.idle":"2023-12-31T17:17:32.029736Z","shell.execute_reply.started":"2023-12-31T17:17:32.021433Z","shell.execute_reply":"2023-12-31T17:17:32.028641Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_sample = df.loc[[5, 211, 434, 6141]]\ndf_sample['labels'] = [\n    [7049, 4850, 5814, 6104, 2180, 8242, 4893, 13032, 3453, 11317, 2966, 13431, 3654, 8186, 6224, 6762],\n    [233, 6498, 9743, 4922, 3673],\n    [13557, 5734, 7203, 1166, 1121, 8793],\n    [1370, 11127, 4544, 3338, 6670, 699, 6667, 521, 680, 6535]\n]\ndf_sample[['Job Description', 'labels']].to_csv(\"df_sample_labeled.csv\")\n\nfor index, s in df_sample.iterrows():\n    esco_skills = [esco_df.loc[x]['label_cleaned'] for x in s['labels']]\n    print(s['Job Title'], '\\n', esco_skills, \"\\n-----------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:32.030834Z","iopub.execute_input":"2023-12-31T17:17:32.031115Z","iopub.status.idle":"2023-12-31T17:17:32.060979Z","shell.execute_reply.started":"2023-12-31T17:17:32.031090Z","shell.execute_reply":"2023-12-31T17:17:32.059791Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Cyber IT Risk & Strategy Senior Consultant \n ['cyber security', 'cyber attack counter-measures', 'implement ICT risk management', 'risk management', 'perform business analysis', 'apply risk management processes', 'perform church service', 'DevOps', \"assess risks of clients' assets\", 'present reports', 'develop information security strategy', 'implement strategic planning', 'identify technological needs', 'consult with business clients', 'project management', 'advise client on technical possibilities'] \n-----------------------------\nAV Systems Drawings AutoCAD Engineer \n ['design electrical systems', 'create AutoCAD drawings', 'use CAD software', 'technical drawings', 'mathematics'] \n-----------------------------\nPre-owned Auto Sales Consultant \n ['advise customers on motor vehicles', \"identify customer's needs\", 'satisfy customers', 'communication', 'Spanish', 'understand spoken Spanish'] \n-----------------------------\nSenior Java Developer \n ['Java', 'Python', 'Groovy', 'JavaScript', 'develop software prototype', 'Apache Maven', 'perform software unit testing', 'systems development life-cycle', 'Ruby', 'NoSQL'] \n-----------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"threshold = .8\npreds = {}\nres_log = {}\nsim_start_time = time.time()\nfor index, s in df_sample.iterrows():\n    res = []\n    pred_labels = []\n    sent_embs = []\n    sentences = get_sentences(s['Job Description'])\n    for sent in sentences:\n        sent_embs.append(get_embedding(sent).squeeze())\n        \n    idxs, sims = compute_similarity_mat(sent_embs, emb_label)\n    for i in range(len(idxs)):\n        if sims[i] > threshold:\n            pred_labels.append(idxs[i])\n            res.append((sentences[i], esco_df.iloc[idxs[i]]['label_cleaned'], sims[i]))\n    # Save results\n    preds[index] = list(set(pred_labels))\n    res_log[index] = list(set(res))\n# Calculate job description processing time\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")\ncalculate_metrics(preds, df_sample['labels'])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:32.883372Z","iopub.execute_input":"2023-12-31T17:17:32.884127Z","iopub.status.idle":"2023-12-31T17:17:34.391969Z","shell.execute_reply.started":"2023-12-31T17:17:32.884093Z","shell.execute_reply":"2023-12-31T17:17:34.391007Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Execution time: 1.4995 seconds\nPrecision: 0.1538, Recall: 0.0541, F1-score: 0.0800\n","output_type":"stream"}]},{"cell_type":"code","source":"out_treshold = .8\nsim_treshold = .8\npreds = {}\nres_log = {}\nstart_time = time.time()\nfor index, s in df_sample.iterrows():\n    pred_labels, res = extract_skills(s['Job Description'], tsc, tkc)\n    # Save results\n    preds[index] = list(set(pred_labels))\n    res_log[index] = list(set(res))\n# Calculate job description processing time\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")\ncalculate_metrics(preds, df_sample['labels'])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:17:34.800471Z","iopub.execute_input":"2023-12-31T17:17:34.801423Z","iopub.status.idle":"2023-12-31T17:17:38.980878Z","shell.execute_reply.started":"2023-12-31T17:17:34.801381Z","shell.execute_reply":"2023-12-31T17:17:38.979891Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Execution time: 4.1726 seconds\nPrecision: 0.3167, Recall: 0.5135, F1-score: 0.3918\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## N-grams extraction\nI also tried splitting the words to at most 4-words long n-grams and extracted skills this way. I tried multiple combinations, selecting all similarities above a given threshold, then remove duplicates or only taking a maximum value per sentences.\n\n**Only max similarity n-gram for each sentence**","metadata":{}},{"cell_type":"code","source":"threshold = .85\npreds = {}\nres_log = {}\n\nsim_start_time = time.time()\nfor index, s in df_sample.iterrows():\n    res = []\n    pred_labels = []\n    sentences = get_sentences(s['Job Description'])\n    \n    for sent in sentences:\n        ngram_embs = []\n        sent_emb = get_embedding(sent).squeeze()\n\n        words = sent.split()\n        ngram_max_len = 4 if len(words) > 4 else len(words)\n        ngram_list = [gram for i in range(1, ngram_max_len+1) for gram in list(ngrams(words, i))]\n        for ngram in ngram_list:\n            text = ' '.join(ngram)\n            ngram_embs.append(get_embedding(text).squeeze()*0.8 + sent_emb*0.2)\n            \n        idxs, sims = compute_similarity_mat(ngram_embs, emb_label)\n        max_idx = np.argmax(sims)\n        if sims[max_idx] > threshold:\n            pred_labels.append(idxs[max_idx])\n            res.append((ngram_list[max_idx], esco_df.iloc[idxs[max_idx]]['label_cleaned'], sims[max_idx]))\n    preds[index] = list(set(pred_labels))\n    res_log[index] = list(set(res))\n        \n# Calculate job description processing time\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")\ncalculate_metrics(preds, df_sample['labels'])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T17:47:30.517789Z","iopub.execute_input":"2023-12-31T17:47:30.518249Z","iopub.status.idle":"2023-12-31T17:48:36.399323Z","shell.execute_reply.started":"2023-12-31T17:47:30.518213Z","shell.execute_reply":"2023-12-31T17:48:36.398285Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Execution time: 65.8697 seconds\nPrecision: 0.1562, Recall: 0.2703, F1-score: 0.1980\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**All similarity n-gram above threshold**","metadata":{}},{"cell_type":"code","source":"threshold = .85\npreds = {}\nres_log = {}\n\nsim_start_time = time.time()\nfor index, s in df_sample.iterrows():\n    res = []\n    pred_labels = []\n    ngram_embs = []\n    ngram_texts = []\n    sentences = get_sentences(s['Job Description'])\n    \n    for sent in sentences:\n        sent_emb = get_embedding(sent).squeeze()\n\n        words = sent.split()\n        ngram_max_len = 4 if len(words) > 4 else len(words)\n        ngram_list = [gram for i in range(ngram_min_len, ngram_max_len+1) for gram in list(ngrams(words, i))]\n        for ngram in ngram_list:\n            text = ' '.join(ngram)\n            ngram_embs.append(get_embedding(text).squeeze()*0.8 + sent_emb*0.2)\n            ngram_texts.append(text)\n            \n    idxs, sims = compute_similarity_mat(ngram_embs, emb_label)\n    for i in range(len(idxs)):\n        if sims[i] > threshold:\n            pred_labels.append(idxs[i])\n            res.append((ngram_texts[i], esco_df.iloc[idxs[i]]['label_cleaned'], sims[i]))\n    preds[index] = list(set(pred_labels))\n    res_log[index] = list(set(res))\n        \n# Calculate job description processing time\nsim_end_time = time.time()\nexecution_time = sim_end_time - sim_start_time\nprint(f\"Execution time: {execution_time:.4f} seconds\")\ncalculate_metrics(preds, df_sample['labels'])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T18:08:02.039380Z","iopub.execute_input":"2023-12-31T18:08:02.039796Z","iopub.status.idle":"2023-12-31T18:08:45.111499Z","shell.execute_reply.started":"2023-12-31T18:08:02.039766Z","shell.execute_reply":"2023-12-31T18:08:45.110550Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Execution time: 43.0602 seconds\nPrecision: 0.1136, Recall: 0.4054, F1-score: 0.1775\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Exploring possible job benefits ontology","metadata":{}},{"cell_type":"code","source":"df_temp = pd.read_csv(\"/kaggle/input/job-description-dataset/job_descriptions.csv\")\ndf_temp = df_temp.loc[:]['Benefits']","metadata":{"execution":{"iopub.status.busy":"2023-12-31T18:11:02.986487Z","iopub.execute_input":"2023-12-31T18:11:02.987523Z","iopub.status.idle":"2023-12-31T18:11:42.551491Z","shell.execute_reply.started":"2023-12-31T18:11:02.987486Z","shell.execute_reply":"2023-12-31T18:11:42.550624Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"benefits = set()\nfor row in df_temp:\n    words = re.sub(r'[\\{\\}\\']', '', row).split(',')\n    for w in words:\n        benefits.add(w)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T18:11:42.553194Z","iopub.execute_input":"2023-12-31T18:11:42.553493Z","iopub.status.idle":"2023-12-31T18:11:42.576315Z","shell.execute_reply.started":"2023-12-31T18:11:42.553467Z","shell.execute_reply":"2023-12-31T18:11:42.575253Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"print(len(benefits))\nbenefits","metadata":{"execution":{"iopub.status.busy":"2023-12-31T18:12:51.935724Z","iopub.execute_input":"2023-12-31T18:12:51.936485Z","iopub.status.idle":"2023-12-31T18:12:51.943837Z","shell.execute_reply.started":"2023-12-31T18:12:51.936453Z","shell.execute_reply":"2023-12-31T18:12:51.942894Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"35\n","output_type":"stream"},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"{' Bonuses and Incentive Programs',\n ' Casual Dress Code',\n ' Childcare Assistance',\n ' Employee Assistance Programs (EAP)',\n ' Employee Discounts',\n ' Employee Recognition Programs',\n ' Employee Referral Programs',\n ' Financial Counseling',\n ' Flexible Spending Accounts (FSAs)',\n ' Flexible Work Arrangements',\n ' Health Insurance',\n ' Health and Wellness Facilities',\n ' Legal Assistance',\n ' Life and Disability Insurance',\n ' Paid Time Off (PTO)',\n ' Parental Leave',\n ' Professional Development',\n ' Profit-Sharing',\n ' Relocation Assistance',\n ' Retirement Plans',\n ' Social and Recreational Activities',\n ' Stock Options or Equity Grants',\n ' Transportation Benefits',\n ' Tuition Reimbursement',\n ' Wellness Programs',\n 'Casual Dress Code',\n 'Childcare Assistance',\n 'Employee Assistance Programs (EAP)',\n 'Employee Referral Programs',\n 'Flexible Spending Accounts (FSAs)',\n 'Health Insurance',\n 'Legal Assistance',\n 'Life and Disability Insurance',\n 'Transportation Benefits',\n 'Tuition Reimbursement'}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}